{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 1) Part of Speech tagging using unigram , Bigram , Trigram and combined all of these**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wRoN8KCFA1m9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  **Unigram**"
      ],
      "metadata": {
        "id": "P0vNZQuqSZpN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mc8rCf9AqTW",
        "outputId": "fe0b5fa6-b5cf-4c57-bdc1-75da381782a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8121200039868434"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        "brown_sents = brown.sents(categories='news')\n",
        "size = int(len(brown_tagged_sents) * 0.9)\n",
        "train_sents = brown_tagged_sents[:size]\n",
        "test_sents = brown_tagged_sents[size:]\n",
        "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
        "unigram_tagger.accuracy(test_sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  **Bigram**"
      ],
      "metadata": {
        "id": "d8rwuFeLScJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_sents)\n",
        "bigram_tagger.accuracy(test_sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MaHnkplOuv0",
        "outputId": "237772c0-19fc-46ef-fdd9-833323d3a4bf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10206319146815508"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Trigram**"
      ],
      "metadata": {
        "id": "lRAOkHwtSeXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_tagger =  nltk.TrigramTagger(train_sents)\n",
        "trigram_tagger.accuracy(test_sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJQUPFx-MSW1",
        "outputId": "7f166c27-ef34-4c85-d10f-72fb83996627"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0626931127279976"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  **Combining Unigram bigram and Trigram**"
      ],
      "metadata": {
        "id": "0c2fKDGQSgp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = nltk.DefaultTagger('NN')\n",
        "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
        "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
        "t3 = nltk.TrigramTagger(train_sents, backoff=t2)\n",
        "t3.accuracy(test_sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4Ul51LgMRjM",
        "outputId": "f861391e-aefd-41c4-b336-3db84531f44f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.843317053722715"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 2)  Demonstration of Regular expression - based NP chunker**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LD7-dFXkBjjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install svgling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FplRAV1cFBn8",
        "outputId": "4cd9544a-851a-4406-9c34-3bfecf998b0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting svgling\n",
            "  Downloading svgling-0.3.1-py3-none-any.whl (21 kB)\n",
            "Collecting svgwrite\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.3.1 svgwrite-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chunk import RegexpParser\n",
        "chunker = RegexpParser(r'''\n",
        "NP:\n",
        "{<DT><NN.*><.*>*<NN.*>}\n",
        "}<VB.*>{\n",
        "''')\n",
        " \n",
        "chunker.parse([('the', 'DT'), ('book', 'NN'), (\n",
        "    'has', 'VBZ'), ('many', 'JJ'), ('chapters', 'NNS')])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "bBmWOcgCBzNN",
        "outputId": "2b68d9b0-048c-4186-d708-f7abbc285c2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP', [('the', 'DT'), ('book', 'NN')]), ('has', 'VBZ'), Tree('NP', [('many', 'JJ'), ('chapters', 'NNS')])])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,256.0,168.0\" width=\"256px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"34.375%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"45.4545%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.7273%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"54.5455%\" x=\"45.4545%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">book</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.7273%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.1875%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"15.625%\" x=\"34.375%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">has</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.1875%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"37.5%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">many</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"18.75%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"62.5%\" x=\"37.5%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">chapters</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 3)  Designing a noun phrase chunker**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AkgWRWfsFLol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkaDMHIff3Tp",
        "outputId": "c71170a0-9772-4f2f-cb01-2709aeb97b49"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk import RegexpParser\n",
        "text =\"I am Vaibhav and this is NLP  example\".split()\n",
        "print(\"After Split:\",text)\n",
        "tokens_tag = pos_tag(text)\n",
        "print(\"After Token:\",tokens_tag)\n",
        "patterns= \"\"\"mychunk:{<NN.?>}\"\"\"\n",
        "chunker = RegexpParser(patterns)\n",
        "print(\"After Regex:\",chunker)\n",
        "output = chunker.parse(tokens_tag)\n",
        "print(\"Noun Chunker\",output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1tclEAqEuVi",
        "outputId": "0e53513d-50cf-4369-d707-8ae669527169"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Split: ['I', 'am', 'Vaibhav', 'and', 'this', 'is', 'NLP', 'example']\n",
            "After Token: [('I', 'PRP'), ('am', 'VBP'), ('Vaibhav', 'NNP'), ('and', 'CC'), ('this', 'DT'), ('is', 'VBZ'), ('NLP', 'JJ'), ('example', 'NN')]\n",
            "After Regex: chunk.RegexpParser with 1 stages:\n",
            "RegexpChunkParser with 1 rules:\n",
            "       <ChunkRule: '<NN.?>'>\n",
            "Noun Chunker (S\n",
            "  I/PRP\n",
            "  am/VBP\n",
            "  (mychunk Vaibhav/NNP)\n",
            "  and/CC\n",
            "  this/DT\n",
            "  is/VBZ\n",
            "  NLP/JJ\n",
            "  (mychunk example/NN))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 4)  Develop a simple context-Free grammar for parsing a English sentence using NLTK**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "53i5NFn7K2ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import CFG\n",
        "grammar1 = CFG.fromstring(\"\"\"\n",
        " S -> NP VP\n",
        " VP -> V NP | V NP PP\n",
        " PP -> P NP\n",
        " V -> \"saw\" | \"ate\" | \"walked\"\n",
        " NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
        " Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
        " N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
        " P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
        " \"\"\")\n",
        "sent = \"Mary saw Bob\".split()\n",
        "parser = nltk.ChartParser(grammar1)\n",
        "for tree in  parser.parse(sent):\n",
        "  print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ooyzo9WmM6gR",
        "outputId": "a1585508-303f-4456-8934-9987a380c287"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NP Mary) (VP (V saw) (NP Bob)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 5)  Design a program for searching for three-word phrases using POS tags**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9RVk6or4NGdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "def process(sentence):\n",
        " for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(sentence):\n",
        "  if (t1.startswith('V') and t2 == 'TO' and t3.startswith('V')):\n",
        "    print(w1, w2, w3)\n",
        "for tagged_sent in brown.tagged_sents()[:100]:\n",
        "  process(tagged_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtDbHlZAMDyD",
        "outputId": "6b242bc0-0e32-4992-d17c-d2875ada5e0b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combined to achieve\n",
            "continue to place\n",
            "serve to protect\n",
            "wanted to wait\n",
            "allowed to place\n",
            "expected to become\n",
            "expected to approve\n",
            "expected to make\n",
            "intends to make\n",
            "seek to set\n",
            "like to see\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 6)  Create a tagger that tags everything as NN**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t9MxfI1Wh2aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJy9o6bw2QEn",
        "outputId": "a924db9a-f9a5-432a-ad4b-83dd781931b0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw = 'I do not like green eggs and ham, I do not like them Sam I am!'\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "default_tagger.tag(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymdO1HGDiFKj",
        "outputId": "6a564077-6099-42da-9ceb-267eaa57b489"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'NN'),\n",
              " ('do', 'NN'),\n",
              " ('not', 'NN'),\n",
              " ('like', 'NN'),\n",
              " ('green', 'NN'),\n",
              " ('eggs', 'NN'),\n",
              " ('and', 'NN'),\n",
              " ('ham', 'NN'),\n",
              " (',', 'NN'),\n",
              " ('I', 'NN'),\n",
              " ('do', 'NN'),\n",
              " ('not', 'NN'),\n",
              " ('like', 'NN'),\n",
              " ('them', 'NN'),\n",
              " ('Sam', 'NN'),\n",
              " ('I', 'NN'),\n",
              " ('am', 'NN'),\n",
              " ('!', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 7)  Design a program to train a tagger by seperating the Training and Testing Data and after that calculate the model accuracy**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hihe9KZ6iGgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj9pHWqX4B-9",
        "outputId": "94528a67-7764-4f05-bd3d-09f6c523818e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        "brown_sents = brown.sents(categories='news')\n",
        "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
        "unigram_tagger.tag(brown_sents[2007])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Gc-zuibihAH",
        "outputId": "7eec9316-d447-461d-9689-35551f5a5763"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Various', 'JJ'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'AT'),\n",
              " ('apartments', 'NNS'),\n",
              " ('are', 'BER'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'AT'),\n",
              " ('terrace', 'NN'),\n",
              " ('type', 'NN'),\n",
              " (',', ','),\n",
              " ('being', 'BEG'),\n",
              " ('on', 'IN'),\n",
              " ('the', 'AT'),\n",
              " ('ground', 'NN'),\n",
              " ('floor', 'NN'),\n",
              " ('so', 'QL'),\n",
              " ('that', 'CS'),\n",
              " ('entrance', 'NN'),\n",
              " ('is', 'BEZ'),\n",
              " ('direct', 'JJ'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " unigram_tagger.accuracy(brown_tagged_sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3Npsx_139U-",
        "outputId": "a9a10a74-4858-444a-8e5c-defd8041a205"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9349006503968017"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size = int(len(brown_tagged_sents) * 0.9)\n",
        "train_sents = brown_tagged_sents[:size]\n",
        "test_sents = brown_tagged_sents[size:]\n",
        "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
        "unigram_tagger.accuracy(test_sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c48okkAN4ehg",
        "outputId": "1f5bb3e0-82c1-4fbe-8ffe-63ba6fa83f1f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8121200039868434"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SrIvJ8pP5LDL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}